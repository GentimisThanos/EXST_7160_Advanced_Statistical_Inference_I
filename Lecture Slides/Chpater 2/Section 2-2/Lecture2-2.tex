\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}

\lstset{language=R,% set programming language
	basicstyle=\small,% basic font style
	keywordstyle=\bfseries,% keyword style
        commentstyle=\ttfamily\itshape,% comment style
	tabsize=3,% sizes of tabs
	showstringspaces=false,% do not replace spaces in strings by a certain character
	captionpos=b,% positioning of the caption below
        breaklines=true,% automatic line breaking
        escapeinside={(*}{*)},% escaping to LaTeX
        fancyvrb=true,% verbatim code is typset by listings
        extendedchars=false,% prohibit extended chars (chars of codes 128--255)
        literate={"}{{\texttt{"}}}1{<-}{{$\leftarrow$}}1{<<-}{{$\twoheadleftarrow$}}1
        {~}{{$\sim$}}1{<=}{{$\le$}}1{>=}{{$\ge$}}1{!=}{{$\neq$}}1{^}{{$^\wedge$}}1,% item to replace, text, length of chars
        alsoletter={.<-},% becomes a letter
        alsoother={$},% becomes other
        otherkeywords={!=, ~, $, *, \&, \%/\%, \%*\%, \%\%, <-, <<-, /},% other keywords
        deletekeywords={c}% remove keywords
}

%	numbers=left,% display line numbers on the left side
%	numberstyle=\scriptsize,% use small line numbers
%	numbersep=10pt,% space between line numbers and code

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title[]{Topic 2-2: Likelihoods for Regression Models}
\author{Department of Experimental Statistics \\
Louisiana State University}
\institute{}
\date{Date}

\setbeamertemplate{footline}[frame number]{}

\begin{document}

\begin{frame}[noframenumbering,plain]
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{General Concept}



\begin{frame}{Motivation Data}
        \begin{itemize}
            \item The annual maximum sea levels in Venice for 1931–1981 are given in Pirazzoli (1982).
            \item The data are as follows:
\begin{table}
\hspace{-1.75 cm}
\begin{tiny}
\centering
\begin{tabular}{|c|ccccccccccccc|}
\hline
Year & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 &  13 \\
Levels & 103 & 78 & 121 & 116 & 115 & 147 & 119 & 114 & 89 & 102 & 99 & 91 & 97 \\\hline
Year & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 \\\
Levels & 106 & 105 & 136 & 126 & 132 & 104 & 117 & 151 & 116 & 107 & 112 & 97 & 95 \\\hline
Year & 27 & 28 & 29 & 30 & 31 & 32 & 33 & 34 & 35 & 36 & 37 & 38 & 39  \\
Levels & 119 & 124 & 118 & 145 & 122 & 114 & 118 & 107 & 110 & 194 & 138 & 144 & 138  \\\hline
Year & 40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 & 48 & 49 & 50 & 51 & \\
Levels & 123 & 122 & 120 & 114 & 96 & 125 & 124 & 120 & 132 & 166 & 134 & 138 & \\\hline
\end{tabular}
\end{tiny}
\end{table}
\item In this dataset, we are interested in building a {\bf regression model} for describing the relationship between Year as a {\it covariate} and See Levels as the {\it response}.
\item What is the main difference between the structure of the dataset and that of datasets in previous section? Do we have response in previous datasets?
        \end{itemize}
    \end{frame}
    
    
\begin{frame}{Linear Models}
        \begin{itemize}
            \item Suppose we observed $p$ covariates from each observation $\boldsymbol{x}_{i}, i = 1, \cdots, n$ and the associated response $y_{i}$. A normal linear model is the most common regression model for describing the relationship between $\boldsymbol{x}_{i}$ and $y_{i}$ that can be expressed as 
            \begin{equation}
               Y_{i} = x^{T}_{i}\beta + e_{i}, \label{eq: normalM}
            \end{equation}
            where $e_{1}, \cdots, e_{n}$ are iid $N(0, \sigma^2)$, and $\boldsymbol{\beta}$ are an unknown $p$-dimensional vector.
            \item The first component of $\boldsymbol{x}_{i}$ is usually the constant “1” corresponding to an intercept, the first component of $\boldsymbol{\beta}$.
            \item In this section, we are interested in constructing likelihood function for unknown parameters $\boldsymbol{\theta} = (\boldsymbol{\beta}, \sigma)$ and extend the normal linear model to non-normal models.
        \end{itemize}
    \end{frame}
%   \item A statistical model is a general functional relation between the unknown parameter(s) and the observed data.
%After a statistical model for the observed data has been formulated, the likelihood function of the data is the natural starting point for the inference in many statistical problems.
%\item The likelihood function typically leads to essentially automatic methods of inference, including point estimation, interval estimation, and hypothesis testing.
    \begin{frame}{Likelihood for Normal Linear Models}
        \begin{itemize}
            \item The likelihood of data $\{\boldsymbol{y}_{i}, \boldsymbol{x}_{i}\}^{n}_{i=1}$ from the normal linear models (\ref{eq: normalM}) is
            \begin{eqnarray}
                L(\boldsymbol{\beta}, \sigma |\{\boldsymbol{y}_{i}, \boldsymbol{x}_{i}\}^{n}_{i=1}) & = & \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y_{i}-\boldsymbol{x}^{T}_{i}\beta)^2}{2\sigma^2}\right) \nonumber \\
                & = & \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^{n}\exp\left(-\sum^{n}_{i=1}\frac{(y_{i}-\boldsymbol{x}^{T}_{i}\beta)^2}{2\sigma^2}\right) \nonumber
            \end{eqnarray}
            \item Based on the likelihood, the maximum likelihood estimator of $\boldsymbol{\beta}$ is $\hat{\boldsymbol{\beta}}_{MLE} = (\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{Y},$
            where $\boldsymbol{X} = (x_{1}, \cdots, x_{n})^{T}$ an $n \times p$ matrix, and $\boldsymbol{X}^{T}\boldsymbol{X}$ is assumed to be nonsigular.
            \item The MLE of $\sigma^2 = \sum^{n}_{i=1} \hat{e}_{i},$
            where $\hat{e}_{i} = y_{i} - \boldsymbol{x}^{T}_{i}\hat{\boldsymbol{\beta}}_{MLE}$ is the residual from $i$-th data for $i = 1, \cdots, n$. (Check the MLEs by yourself!)
        \end{itemize}
    \end{frame}


    \begin{frame}{Nonnormal Error Models}
        \begin{itemize}
            \item Instead of the assumption of putting the error term from model (\ref{eq: normalM}) following a  normal distribution, we may assume the error term follows other distributions.
            \item A popular choice is to assume $e_{i}$ following a scale-family distribution whose density is \begin{equation}
            \frac{1}{\sigma}f_{e}\left(\frac{y_{i} - \boldsymbol{x}^{T}_{i}\boldsymbol{\theta}}{\sigma}\right);
            \end{equation}
            for example, the extreme value density is one of the distribution in the scale family and its density is $f_{e}(t) = \exp(-t) \times \exp(-\exp(-t)).$
        \item The likelihood function becomes
        $$\prod^{n}_{i=1}\frac{1}{\sigma}f_{e}\left(\frac{y_{i} - \boldsymbol{x}^{T}_{i}\boldsymbol{\theta}}{\sigma}\right)$$
        \end{itemize}
    \end{frame}
    
        \begin{frame}{Revisit the Sea Level data}
        \begin{itemize}
          \item
        \end{itemize}
    \end{frame}
    
    
            \begin{frame}{R Code}
        \begin{itemize}
           \item
        \end{itemize}
    \end{frame}
    



    \begin{frame}{Additive Errors Nonlinear Model}
        \begin{itemize}
            \item The standard nonlinear regression model is very similar to (\ref{eq: normalM}) but considering $Y_{i} = g(\boldsymbol{x}_{i}, \boldsymbol{\beta}) + e_{i},$
            where $g$ is a known function with unknown parameters $\boldsymbol{\beta}$.
            \item Common examples include 
            \begin{itemize}
                \item Exponential growth model $g(\boldsymbol{x}_{i}, \boldsymbol{\beta}) = \beta_{0}\exp(\beta_{1}x_{i})$
                \item logistic growth model $g(x_{i}, \boldsymbol{\beta}) = \beta_{0}(1+\beta_{1}\exp(-\beta_{2}x_{i}))$
            \end{itemize}
            \item The MLE is from maximizing
             \begin{eqnarray}
                L(\boldsymbol{\beta}, \sigma |\{\boldsymbol{y}_{i}, \boldsymbol{x}_{i}\}^{n}_{i=1}) 
                & = & \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^{n}\exp\left(-\sum^{n}_{i=1}\frac{(y_{i}-{{\color{blue} g(x_{i}, \boldsymbol{\beta}})})^2}{2\sigma^2}\right) \nonumber
            \end{eqnarray}
            \item The MLE of $\boldsymbol{\beta}$ has no closed form but the MLE of $\sigma^2$ is  
            $\sum^{n}_{i=1} \hat{e}_{i}$,
            where $\hat{e}_{i} = y_{i} - \color{blue} g(x_{i}, \hat{\boldsymbol{\beta}}_{MLE})$ for $i = 1, \cdots, n.$
        \end{itemize}
    \end{frame}
    
     \begin{frame}{Motivation Data for GLM}
        \begin{itemize}
            \item 
        \end{itemize}
    \end{frame}
    
    
    \begin{frame}{Generalized Linear models}
        \begin{itemize}
            \item Generalized linear models introduced by Nelder and Wedderburn (1972) are another
important class of nonlinear models that generalizes the normal linear model. It assumes the log density of $Y_{i}$ with parameters $\theta_{i}$ possess the form:
            \begin{equation}
                \log f(y_{i};\theta_{i}, \phi) = \frac{y_{i}\theta_{i}-b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi). \label{eq: LikeexpoF}
            \end{equation}
            \item $a_{i}(\cdot)$ is a known function  and $\phi$ is possibly an unknown parameter called dispersion parameters.
            \item Densities satisfying form (\ref{eq: LikeexpoF}) belong a generalized exponential family. For the students who are not familiar with exponential faimily, please check Appendix B of Section 2 in the Textbook.
            \item GLM can be used for regression modeling of various type responses, including continuous and categorical responses.
            
            
            
        \end{itemize}
    \end{frame}
    %The density of $Y_{i}$ is almost an exponential family density except for the dispersion term $a_{i}(\phi)$, where $a_{i}$ is a known function and $\phi$ is possibly an unknown parameter. In exponential family language, $\theta_{i}$ is called the natural or canonical parameter. (Explain this language)
    
    %  \begin{frame}{Generalized Linear models}
    %  Generalized linear models are introduced by Nelder and Wedderburn (1972). They are another important class of nonlinear models that generalizes the normal linear model and is made up of a linear predictor $$\eta_{i} = \boldsymbol{x}^{T}_{i}\beta$$ and two functions:
    %    \begin{itemize}
    %    \item A link function that describes how the mean, $\mu = E(Y_{i})$,  depends on the linear predictor $$ g(\mu_{i}) = \eta_{i} = \boldsymbol{x}^{T}_{i}\boldsymbol{\beta}$$
    %    \item A variance function that describes how the variance depends on the mean
    %    $$Var(Y_{i}) = V(\mu)a(\phi),$$
    %    where $a(\phi)$ is a function of dispersion parameter $\phi$. 
    %    \end{itemize}
    %\end{frame}
    
    
                  \begin{frame}{Example of the Exponential Family: Normal Distribution}
        \begin{itemize}
            \item Consider $Y$ is a continuous random variable the normal density
            \begin{eqnarray}
                 f(y; \mu, \sigma) & = & -\log\left(\sqrt{2\pi}\sigma\right) - \frac{(y-\mu)^2}{2\sigma^2} \nonumber\\
                 & = & \frac{y\mu-\mu^2/2}{\sigma^2} - \log\left(\sqrt{2\pi}\sigma\right) - \frac{y^2}{2\sigma^2}.
            \end{eqnarray}
            \item Thus,
            $$
            \theta_{i} = \mu_{i}, b(\theta_{i}) = \frac{\mu^2_{i}}{2}, a_{i}(\phi) = \sigma^2,\ \mbox{and}\ c(y_{i}, \phi) = \log(\sigma\sqrt{2\pi})-\frac{y^2_{i}}{2\sigma^2}.
            $$
        \end{itemize}
    \end{frame}
       

           \begin{frame}{Example of the Exponential Family: Bernoulli Distribution}
        \begin{itemize}
            \item Consider $Y$ is a binary random variable from Bernoulli density
            $$f(y; p) = p^{y} (1-p)^{1-y},$$
            where $E(Y) = p$.
           \item Because 
           $$\log f(y; p) = y \log p + (1-y)\log(1-p) = y\log (\frac{p}{1-p}) + \log(1-p),$$
           we obtain 
           $a_{i}(\phi) = 1, c(y_{i}, \phi) = 0, \theta_{i} = \log\{\frac{p_{i}}{1-p_{i}}\}.$
           \item Hence,  $p_{i} = 1/\{1+\exp(-\theta_{i})\},$ so that
             $$b(\theta_{i}) = -\log(1-p_{i}) = -\log\left\{-\frac{1}{1+\exp(-\theta_{i})}\right\} = \log\{1+\exp(\theta_{i})\}.$$
        \end{itemize}
    \end{frame}
    
               \begin{frame}{Example of the Exponential Family: Poisson Distribution}
        \begin{itemize}
            \item Consider $Y$ is a random variable from Poisson density
            $$f(y; p) = \frac{ \theta^{y} e^{-\lambda}  }{y!},$$
            where $E(Y) = \theta$.
           \item Because 
           $$\log f(y; p) =  y\log\lambda - \lambda - \log(y!),$$
           we obtain 
           $$\theta_{i} = \log(\lambda_{i}), b(\theta_{i}) = e^{\theta_{i}} = \lambda_{i}, a_{i}(\phi) = 1, c(y_{i}, \phi) = - \log(y_{i}!).$$
        \end{itemize}
    \end{frame}
    
     
        \begin{frame}{Mean and Variance of the Exponential Family}
        \begin{itemize}
            \item For $Y_{i}$ from a general exponential faimily dentisty (\ref{eq: LikeexpoF})
                $$\log f(y_{i};\theta_{i}, \phi) = \frac{y_{i}\theta_{i}-b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi),$$
                its mean and variance satisfies the following relationships:
                \begin{itemize}
                    \item $\mu = E(Y_{i}) = b'(\theta_{i})$
                    \item $Var(Y_{i}) = b''(\theta_{i})a_{i}(\phi).$\\
                    Note that since the variance must be positive, $b(\theta_{i})$ is a strictly convex function and $b'(\theta_{i})$ is monotone increasing with a unique inverse $b'^{-1}$.
                \end{itemize}
            \item  The relationship is obtained from the fact that
            \begin{itemize}
                \item $E\left\{\frac{\partial}{\partial \theta_{i}}\log f(Y_{i}; \theta_{i}, \phi)\right\} = 0$
                \item $E\left\{\frac{\partial}{\partial \theta_{i}}\log f(Y_{i}; \theta_{i}, \phi)\right\}^2 = E\left\{\frac{\partial^2}{\partial \theta_{i}^2}\log f(Y_{i}; \theta_{i}, \phi)\right\} = 0.$\\
                See page 54 of the textbook for details.
            \end{itemize}
        \end{itemize}
        \end{frame}


     
        \begin{frame}{Canonical Link Functions for GLM}
        \begin{itemize}
            \item Fro a GLM, besides selecting a distribution to model response variable $Y$, we need to choose a link function $g(\cdot)$ to connect the mean of $Y$ with the linear predictor $\boldsymbol{x}_{i}\boldsymbol{\beta},$ i.e.,
            $$g(E(Y)) =  \boldsymbol{x}_{i}\boldsymbol{\beta}$$
            \item In general, the link can be any function who is invertible. Then, for Y is from density (\ref{eq: LikeexpoF}), we know
            $$g(b'(\theta_{i})) = g(E(Y_{i})) = \boldsymbol{x}_{i}\boldsymbol{\beta}.$$
            \item If we further choose $g(\cdot) =  b'^{-1}(\cdot),$  then
            $$g(E(Y_{i})) = g(b'(\theta_{i})) = \theta_{i} = \boldsymbol{x}_{i}\boldsymbol{\beta}$$
            \item The link $g(\cdot) = b'^{-1}(\cdot)$ is called the canonical link.
            \item Canonical links lead to desirable statistical properties of the GLM, so it tends to be used as a default link in GLM.  
            
        \end{itemize}
        \end{frame}
        
        
        
        \begin{frame}{Examples of Canonical Links}
         \begin{table}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
        \hline
        Density of $Y$ & $E(Y)$ & $\theta$ & $g(E(Y))$ \\
                \hline
        Normal & $E(Y) = \mu$ & $\theta = \mu$ & $ \mu = \boldsymbol{x}^{T}\boldsymbol{\beta}$ \\
        Bernoulli & $E(Y) = p$ & $\theta = \log(\frac{p}{1-p})$ & $\log(\frac{p}{1-p}) = \boldsymbol{x}^{T}\boldsymbol{\beta}$ \\
        Poisson & $E(Y) = \lambda$ & $\theta = \log(\lambda)$ & $ \log(\lambda) = \boldsymbol{x}^{T}\boldsymbol{\beta}$ \\
                \hline
        \end{tabular}
        \end{table}
        \end{frame}

        
        \begin{frame}{Likelihood for the GLM}
        \begin{itemize}
            \item By using the canonical link in GLM, the log likelihood of the dataset $\{(y_{i}, x_{i})\}^{n}_{i=1}$ is
            $$\log L(\boldsymbol{\beta}, \phi|\{y_{i}, \boldsymbol{x}_{i}\}^{n}_{i=1}) \sum^{n}_{i=1}\left\{\frac{y_{i}\boldsymbol{x}^{T}_{i}\boldsymbol{\beta} - b(\boldsymbol{x}^{T}_{i}\boldsymbol{\beta})}{a_{i}(\phi)} + c(y_{i}, \phi)\right\}$$
            
        \end{itemize} 
        \end{frame}
        
        \begin{frame}{Generalized Linear Mixed Model (GLMM)}
        \begin{itemize}
            \item So far we have considered only fixed effect models in which the input $\boldsymbol{x}$ are fixed in advance. For example, in the Venice dataset, the input year is "fixed" from 1 to 51. If instead the year effect is said to have arisen from a statistical distribution, then we have a {\bf random effects} model. 
            
            \item GLMM extends GLM to include not only fixed effects but also random effect terms in the inputs. Such model considers $Y_{i}$ whose log density can be expressed as
            $$\log f_{Y_{i}|\boldsymbol{U}}(y_{i}|\boldsymbol{U}, \boldsymbol{x}_{i}, \boldsymbol{z}_{i}, \boldsymbol{\beta}, \phi) = \frac{y_{i}\theta_{i} - b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi)
            \label{eq: dGLMM}$$
            and
            $$\theta_{i} = \boldsymbol{x}^{T}_{i}\boldsymbol{\beta} + \boldsymbol{z}^{T}_{i}\boldsymbol{U},$$
            where $\boldsymbol{U}$ is a random effect vector term following a density $f_{\boldsymbol{U}}(\boldsymbol{u}|\boldsymbol{v})$ with parameters $\boldsymbol{v}$ and $\boldsymbol{z}_{i}$ are known nonrandom predictors

        \end{itemize} 
        \end{frame}

        \begin{frame}{GLMM (Cont.)}
        \begin{itemize}
            \item In the normal distribution case, GLMM becomes the usual linear mixed model $\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} +  \boldsymbol{Z}\boldsymbol{U} + \boldsymbol{e},$ where $\mathbf{X} = (\mathbf{x}^{T}_{1}, \cdots, \mathbf{x}^{T}_{n} )$ and $\mathbf{Z} = (\mathbf{z}^{T}_{1}, \cdots, \mathbf{z}^{T}_{n}).$ 
            \item Suppose $Y_{1}, \cdots, Y_{n}$ are conditionally independent from (\ref{eq: dGLMM}), then the likelihood of GLMM is
            $$\hspace{- 0.5 cm} L(\boldsymbol{\beta}, \phi, \boldsymbol{v}|\{y_{i}, x_{i}, z_{i}\}^{n}_{i=1}) = \int \prod^{n}_{i=1} \log f_{Y_{i}|\boldsymbol{U}}(y_{i}|\boldsymbol{U}, \boldsymbol{x}_{i}, \boldsymbol{z}_{i}, \boldsymbol{\beta}, \phi)f_{\boldsymbol{U}}(\boldsymbol{u}|\boldsymbol{v})d\boldsymbol{u}
            $$
            \item Note integration over $\boldsymbol{u}$ is required because $\mathbf{U}$ is not ovserved, and the likelihood is the density of the observed data.
            \item In general, the likelihood is not easy to calculate or maximize. Breslow and Clayton (1993) and McCullagh (1997) give some of the important approaches for finding the maximum likelihood estimators, but this remains an active area of research. 
        \end{itemize} 
        \end{frame}



       \begin{frame}{Accelerated Failure Model}
        \begin{itemize}
            \item Accelerated Failure Models comprise an important class of regression models for censored data. For the ease of random right censoring, the model is 
            $$\log T_{i} = \boldsymbol{x}^{T}_{i}\boldsymbol{\beta}, $$
            where we observe $Y_{i} = \min\left(\log T_{i}, \log R_{i}\right),$ and $R_{i}$ is a censoring time that is assumed independent of $T_{t}$
            \item The typical models for the errors $e_{i}$ are standard normal, logistic, or the logatithm of a standard exponential random variable whose density is $f_{e} = e^{z}e^{-e^{z}}$.
            \item The likelihood for parameters $\boldsymbol{\beta}$ and $\sigma$ with using error density $f_{e}$ and distribution $F_{e}$ is 
            $$L(\boldsymbol{\beta}, \sigma|\{y_{i}, \delta_{i}, \boldsymbol{x}_{i}\}^{n}_{i=1}) = \prod^{n}_{i=1}\left[\frac{1}{\sigma}f_{e}(r_{i})\right]^{\delta_{i}}\left[1 - F_{e}(r_{i})\right]^{\delta_{i}},$$
            where $\delta_{i} = I(y_{i} = \log t_{i})$ and $r_{i} = (y_{i} - \boldsymbol{x}^{T}_{i}\boldsymbol{\beta})/\sigma.$ 
           
        \end{itemize}
    \end{frame}
    
           \begin{frame}{Example for Accelerated Failure Model}
        \begin{itemize}
            \item 
           
        \end{itemize}
    \end{frame}



\end{document}


         \begin{frame}{Example 1 Normal Linear Models}
         \begin{itemize}
         \item In normal linear model (\ref{eq: normalM}), $Y_{i} \sim N(\boldsymbol{x}^{T}_{i}\boldsymbol{\beta}, \sigma^{2})$.
        \item The link function is identity link function, i.e. $g(\cdot) = 1$, and
        $$\mu = \boldsymbol{x}^{T}_{i}\boldsymbol{\beta}$$
        \item The variance function is $V(\mu) = 1$ and dispersion parameter function $a(\phi) = \sigma^2$.
        \end{itemize}
        \end{frame}
        
        
        \begin{frame}{Example 2: Logistic Linear Models}
        \begin{itemize}
         \item For binary data, $Y_{i}$ is following a Bernoulli distribution with parameter $E(Y_{i}) = p_{i}$ for $i = 1, \cdots, n$.
        \item A popular link function is called logistic link, $g(p_{i}) = \log\left(\frac{p_{i}}{1-p_{i}}\right),$ i.e, 
        $$\log\left(\frac{p_{i}}{1-p_{i}}\right) = \boldsymbol{x}^{T}_{i}\boldsymbol{\beta}$$
        \item Variance function $V(p_{i}) = p_{i}(1-p_{i})$ and $a_{i}(\phi) = 1$ (because $Var(Y_{i})) = p_{i}(1-p_{i})$
        \end{itemize}
        \end{frame}
        
        
        \begin{frame}{General Exponential Family}
        \begin{itemize}
         \item The normal and bernoulli density belongs to a family of density functions called general exponential family. If $Y_{i}$ is from the family, its log density can be expressed as
         \begin{equation}
                \log f(y_{i};\theta_{i}, \phi) = \frac{y_{i}\theta_{i}-b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi), \label{eq: LikeexpoF}
        \end{equation}
        where $a_{i}$ is a known function of dispersion parameter $\phi$.
        \end{itemize}
        \end{frame}