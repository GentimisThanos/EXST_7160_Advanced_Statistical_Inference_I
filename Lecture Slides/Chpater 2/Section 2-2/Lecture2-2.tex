\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}

\lstset{language=R,% set programming language
	basicstyle=\small,% basic font style
	keywordstyle=\bfseries,% keyword style
        commentstyle=\ttfamily\itshape,% comment style
	tabsize=3,% sizes of tabs
	showstringspaces=false,% do not replace spaces in strings by a certain character
	captionpos=b,% positioning of the caption below
        breaklines=true,% automatic line breaking
        escapeinside={(*}{*)},% escaping to LaTeX
        fancyvrb=true,% verbatim code is typset by listings
        extendedchars=false,% prohibit extended chars (chars of codes 128--255)
        literate={"}{{\texttt{"}}}1{<-}{{$\leftarrow$}}1{<<-}{{$\twoheadleftarrow$}}1
        {~}{{$\sim$}}1{<=}{{$\le$}}1{>=}{{$\ge$}}1{!=}{{$\neq$}}1{^}{{$^\wedge$}}1,% item to replace, text, length of chars
        alsoletter={.<-},% becomes a letter
        alsoother={$},% becomes other
        otherkeywords={!=, ~, $, *, \&, \%/\%, \%*\%, \%\%, <-, <<-, /},% other keywords
        deletekeywords={c}% remove keywords
}

%	numbers=left,% display line numbers on the left side
%	numberstyle=\scriptsize,% use small line numbers
%	numbersep=10pt,% space between line numbers and code

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title[]{Topic 2-2: Likelihoods for Regression Models}
\author{Department of Experimental Statistics \\
Louisiana State University}
\institute{}
\date{Date}

\setbeamertemplate{footline}[frame number]{}

\begin{document}

\begin{frame}[noframenumbering,plain]
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{General Concept}



\begin{frame}{Motivation Data}
        \begin{itemize}
            \item The annual maximum sea levels in Venice for 1931–1981 are given in Pirazzoli (1982).
            \item The data are as follows:
\begin{table}
\hspace{-1.75 cm}
\begin{tiny}
\centering
\begin{tabular}{|c|ccccccccccccc|}
\hline
Year & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 &  13 \\
Levels & 103 & 78 & 121 & 116 & 115 & 147 & 119 & 114 & 89 & 102 & 99 & 91 & 97 \\\hline
Year & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 \\\
Levels & 106 & 105 & 136 & 126 & 132 & 104 & 117 & 151 & 116 & 107 & 112 & 97 & 95 \\\hline
Year & 27 & 28 & 29 & 30 & 31 & 32 & 33 & 34 & 35 & 36 & 37 & 38 & 39  \\
Levels & 119 & 124 & 118 & 145 & 122 & 114 & 118 & 107 & 110 & 194 & 138 & 144 & 138  \\\hline
Year & 40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 & 48 & 49 & 50 & 51 & \\
Levels & 123 & 122 & 120 & 114 & 96 & 125 & 124 & 120 & 132 & 166 & 134 & 138 & \\\hline
\end{tabular}
\end{tiny}
\end{table}
\item In this dataset, we are interested in building a {\bf regression model} for describing the relationship between Year as a {\it covariate} and See Levels as the {\it response}.
\item What is the main difference between the structure of the dataset and that of datasets in previous section? Do we have response in previous datasets?
        \end{itemize}
    \end{frame}
    
    
\begin{frame}{Linear Models}
        \begin{itemize}
            \item Suppose we observed $p$ covariates from each observation $\mathbf{x}_{i}, i = 1, \cdots, n$ and the associated response $y_{i}$. A normal linear model is the most common regression model for describing the relationship between $\mathbf{x}_{i}$ and $y_{i}$ that can be expressed as 
            \begin{equation}
               Y_{i} = x^{T}_{i}\beta + e_{i}, \label{eq: normalM}
            \end{equation}
            where $e_{1}, \cdots, e_{n}$ are iid $N(0, \sigma^2)$, and $\boldsymbol{\beta}$ are an unknown $p$-dimensional vector.
            \item The first component of $\mathbf{x}_{i}$ is usually the constant “1” corresponding to an intercept, the first component of $\boldsymbol{\beta}$.
            \item In this section, we are interested in constructing likelihood function for unknown parameters $\boldsymbol{\theta} = (\boldsymbol{\beta}, \sigma)$ and extend the normal linear model to non-normal models.
        \end{itemize}
    \end{frame}
%   \item A statistical model is a general functional relation between the unknown parameter(s) and the observed data.
%After a statistical model for the observed data has been formulated, the likelihood function of the data is the natural starting point for the inference in many statistical problems.
%\item The likelihood function typically leads to essentially automatic methods of inference, including point estimation, interval estimation, and hypothesis testing.
    \begin{frame}{Likelihood for Normal Linear Models}
        \begin{itemize}
            \item The likelihood of data $\{\mathbf{y}_{i}, \mathbf{x}_{i}\}^{n}_{i=1}$ from the normal linear models (\ref{eq: normalM}) is
            \begin{eqnarray}
                L(\boldsymbol{\beta}, \sigma |\{\mathbf{y}_{i}, \mathbf{x}_{i}\}^{n}_{i=1}) & = & \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y_{i}-\mathbf{x}^{T}_{i}\beta)^2}{2\sigma^2}\right) \nonumber \\
                & = & \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^{n}\exp\left(-\sum^{n}_{i=1}\frac{(y_{i}-\mathbf{x}^{T}_{i}\beta)^2}{2\sigma^2}\right) \nonumber
            \end{eqnarray}
            \item Based on the likelihood, the maximum likelihood estimator of $\boldsymbol{\beta}$ is $\hat{\boldsymbol{\beta}}_{MLE} = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{Y},$
            where $\mathbf{X} = (x_{1}, \cdots, x_{n})^{T}$ an $n \times p$ matrix, and $\mathbf{X}^{T}\mathbf{X}$ is assumed to be nonsigular.
            \item The MLE of $\sigma^2 = \sum^{n}_{i=1} \hat{e}_{i},$
            where $\hat{e}_{i} = y_{i} - \mathbf{x}^{T}_{i}\hat{\boldsymbol{\beta}}_{MLE}$ is the residual from $i$-th data for $i = 1, \cdots, n$. (Check the MLEs by yourself!)
        \end{itemize}
    \end{frame}


    \begin{frame}{Nonnormal Error Models}
        \begin{itemize}
            \item Instead of the assumption of putting the error term from model (\ref{eq: normalM}) following a  normal distribution, we may assume the error term follows other distributions.
            \item A popular choice is to assume $e_{i}$ following a scale-family distribution whose density is \begin{equation}
            \frac{1}{\sigma}f_{e}\left(\frac{y_{i} - \mathbf{x}^{T}_{i}\boldsymbol{\theta}}{\sigma}\right);
            \end{equation}
            for example, the extreme value density is one of the distribution in the scale family and its density is $f_{e}(t) = \exp(-t) \times \exp(-\exp(-t)).$
        \item The likelihood function becomes
        $$\prod^{n}_{i=1}\frac{1}{\sigma}f_{e}\left(\frac{y_{i} - \mathbf{x}^{T}_{i}\boldsymbol{\theta}}{\sigma}\right)$$
        \end{itemize}
    \end{frame}
    
        \begin{frame}{Revisit the Sea Level data}
        \begin{itemize}
          \item
        \end{itemize}
    \end{frame}
    
    
            \begin{frame}{R Code}
        \begin{itemize}
           \item
        \end{itemize}
    \end{frame}
    



    \begin{frame}{Additive Errors Nonlinear Model}
        \begin{itemize}
            \item The standard nonlinear regression model is very similar to (\ref{eq: normalM}) but considering $Y_{i} = g(\mathbf{x}_{i}, \boldsymbol{\beta}) + e_{i},$
            where $g$ is a known function with unknown parameters $\boldsymbol{\beta}$.
            \item Common examples include 
            \begin{itemize}
                \item Exponential growth model $g(\mathbf{x}_{i}, \boldsymbol{\beta}) = \beta_{0}\exp(\beta_{1}x_{i})$
                \item logistic growth model $g(x_{i}, \boldsymbol{\beta}) = \beta_{0}(1+\beta_{1}\exp(-\beta_{2}x_{i}))$
            \end{itemize}
            \item The MLE is from maximizing
             \begin{eqnarray}
                L(\boldsymbol{\beta}, \sigma |\{\mathbf{y}_{i}, \mathbf{x}_{i}\}^{n}_{i=1}) 
                & = & \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^{n}\exp\left(-\sum^{n}_{i=1}\frac{(y_{i}-{{\color{blue} g(x_{i}, \boldsymbol{\beta}})})^2}{2\sigma^2}\right) \nonumber
            \end{eqnarray}
            \item The MLE of $\boldsymbol{\beta}$ has no closed form but the MLE of $\sigma^2$ is  
            $\sum^{n}_{i=1} \hat{e}_{i}$,
            where $\hat{e}_{i} = y_{i} - \color{blue} g(x_{i}, \hat{\boldsymbol{\beta}}_{MLE})$ for $i = 1, \cdots, n.$
        \end{itemize}
    \end{frame}
    
     \begin{frame}{Motivation Data for GLM}
        \begin{itemize}
            \item 
        \end{itemize}
    \end{frame}
    
    
    \begin{frame}{Generalized Linear models}
        \begin{itemize}
            \item Generalized linear models introduced by Nelder and Wedderburn (1972) are another
important class of nonlinear models that generalizes the normal linear model.
            
            \item It assumes the log density of $Y_{i}$ possess the form:
            \begin{equation}
                \log f(y_{i};\theta_{i}, \phi) = \frac{y_{i}\theta_{i}-b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi). \label{eq: LikeexpoF}
            \end{equation}
            \item The density of $Y_{i}$ is almost an exponential family density except for the dispersion
term $a_{i}(\phi)$, where $a_{i}$ is a known function and $\phi$ is possibly an unknown parameter. In exponential family language, $\theta_{i}$ is called the natural or canonical parameter. (Explain this language)
        \end{itemize}
    \end{frame}
    
    
      \begin{frame}{Generalized Linear models}
      Generalized linear models are introduced by Nelder and Wedderburn (1972). They are another important class of nonlinear models that generalizes the normal linear model and is made up of a linear predictor $$\eta_{i} = \mathbf{x}^{T}_{i}\beta$$ and two functions:
        \begin{itemize}
        \item A link function that describes how the mean, $\mu = E(Y_{i})$,  depends on the linear predictor $$ g(\mu_{i}) = \eta_{i} = \mathbf{x}^{T}_{i}\boldsymbol{\beta}$$
        \item A variance function that describes how the variance depends on the mean
        $$Var(Y_{i}) = V(\mu)a(\phi),$$
        where $a(\phi)$ is a function of dispersion parameter $\phi$. 
        \end{itemize}
    \end{frame}
    
     
         \begin{frame}{Example 1 Normal Linear Models}
         \begin{itemize}
         \item In normal linear model (\ref{eq: normalM}), $Y_{i} \sim N(\mathbf{x}^{T}_{i}\boldsymbol{\beta}, \sigma^{2})$.
        \item The link function is identity link function, i.e. $g(\cdot) = 1$, and
        $$\mu = \mathbf{x}^{T}_{i}\boldsymbol{\beta}$$
        \item The variance function is $1$.
        \end{itemize}
        \end{frame}
        
        
        \begin{frame}{Example 2: Logistic Linear Models}
        \begin{itemize}
         \item For binary data, $Y_{i}$ is following a Bernoulli distribution with parameter $E(Y_{i}) = p_{i}$ for $i = 1, \cdots, n$.
        \item A popular link function is called logistic link, $g(p_{i}) = \log\left(\frac{p_{i}}{1-p_{i}}\right),$ i.e, 
        $$\log\left(\frac{p_{i}}{1-p_{i}}\right) = \mathbf{x}^{T}_{i}\boldsymbol{\beta}$$
        \item Variance function $V(p_{i}) = p_{i}(1-p_{i})$ and $a_{i}(\phi) = 1$ (because $Var(Y_{i})) = p_{i}(1-p_{i})$
        \end{itemize}
        \end{frame}
        
        
        \begin{frame}{General Exponential Family}
        \begin{itemize}
         \item The normal and bernoulli density belongs to a family of density functions called general exponential family. If $Y_{i}$ is from the family, its log density can be expressed as
         \begin{equation}
                \log f(y_{i};\theta_{i}, \phi) = \frac{y_{i}\theta_{i}-b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi), \label{eq: LikeexpoF}
        \end{equation}
        where $a_{i}$ is a known function of dispersion parameter $\phi$.
        \end{itemize}
        \end{frame}
        
               \begin{frame}{Example of Exponential Family: Normal Distribution}
        \begin{itemize}
            \item Consider the normal density
            \begin{eqnarray}
                 f(y; \mu, \sigma) & = & -\log\left(\sqrt{2\pi}\sigma\right) - \frac{(y-\mu)^2}{2\sigma^2} \nonumber\\
                 & = & \frac{y\mu-\mu^2/2}{\sigma^2} - \log\left(\sqrt{2\pi}\sigma\right) - \frac{y^2}{2\sigma^2}.
            \end{eqnarray}
            \item Thus,
            $$
            \theta = \mu, b(\theta) = \frac{\mu^2}{2}, a_{i}(\phi) = \sigma^2,\ \mbox{and}\ c(y_{i}, \phi) = \log(\sigma\sqrt{2\pi})-\frac{y^2}{2\sigma^2}.
            $$
        \end{itemize}
    \end{frame}
       

           \begin{frame}{Example of Exponential Family: Bernoulli Distribution}
        \begin{itemize}
            \item Consider the Bernoulli density
            $$f(y; p) = p^{y} (1-p)^{1-y}$$
            and we want to show that the density is one of the densities in the family (\ref{eq: LikeexpoF}).
           \item Because 
           $$\log f(y; p) = y \log p + (1-y)\log(1-p) = y\log (\frac{p}{1-p}) + \log(1-p),$$
           we obtain 
           $a_{i}(\phi) = 1, c(y_{i}, \phi) = 0, theta = \log\{\frac{p}{1-p}\}.$
           \item Hence,  $p = 1/\{1+\exp(-\theta)\},$ so that
             $$b(\theta) = -\log(1-p) = -\log\left\{-\frac{1}{1+\exp(-\theta)}\right\} = \log\{1+\exp(\theta)\}.$$
         
        \end{itemize}
    \end{frame}
    

    
    
     
        \begin{frame}{Mean and Variance of the Exponential Family}
        \begin{itemize}
            \item For $Y_{i}$ from a general exponential faimily dentisty (\ref{eq: LikeexpoF})
                $$\log f(y_{i};\theta_{i}, \phi) = \frac{y_{i}\theta_{i}-b(\theta_{i})}{a_{i}(\phi)} + c(y_{i}, \phi),$$
                its mean and variance satisfies the following relationships:
                \begin{itemize}
                    \item $\mu = E(Y_{i}) = b'(\theta_{i})$
                    \item $Var(Y_{i}) = b''(\theta_{i})a_{i}(\phi).$\\
                    Note that since the variance must be positive, $b(\theta_{i})$ is a strictly convex function and $b'(\theta_{i})$ is monotone increasing with a unique inverse $b'^{-1}$.
                \end{itemize}
            \item  The relationship is obtained from the fact that
            \begin{itemize}
                \item $E\left\{\frac{\partial}{\partial \theta_{i}}\log f(Y_{i}; \theta_{i}, \phi)\right\} = 0$
                \item $E\left\{\frac{\partial}{\partial \theta_{i}}\log f(Y_{i}; \theta_{i}, \phi)\right\}^2 = E\left\{\frac{\partial^2}{\partial \theta_{i}^2}\log f(Y_{i}; \theta_{i}, \phi)\right\} = 0.$\\
                See page 54 of the textbook for details.
            \end{itemize}
        \end{itemize}
        \end{frame}


     
        \begin{frame}{Canonical Link Functions for GLM}
        \begin{itemize}
            \item Fro a GLM where the response follows a general exponential distribution, we have
            $$g(\mu_{i}) = b'^{-1}(\mu_{i}) = \mathbf{x}^{T}_{i}\boldsymbol{\beta}$$
            \item The link $g(\cdot) = b'^{-1}(\cdot)$ is called the canonical link.
            \item Canonical links lead to desirable statistical properties of the GLM, so it tends to be used as a default link.  
            
        \end{itemize}
        \end{frame}
        
        
        \begin{frame}{Likelihood for the GLM}
        \begin{itemize}
            \item 
            $$\log L(\boldsymbol{\beta}, \phi|\{y_{i}, \mathbf{x}_{i}\}^{n}_{i=1}) \sum^{n}_{i=1}\left\{\frac{y_{i}\mathbf{x}^{T}_{i}\boldsymbol{\beta} - b(\mathbf{x}^{T}_{i}\boldsymbol{\beta})}{a_{i}(\phi)} + c(y_{i}, \phi)\right\}$$
            
        \end{itemize} 
        \end{frame}
        
        \begin{frame}{Examples}
        \begin{itemize}
            \item 
        \end{itemize} 
        \end{frame}


        
        \begin{frame}{Generalized Linear Mixed Model}
        \begin{itemize}
            \item
            
            $$\log f_{Y_{i}|U_{i}}(y_{i}|\mathbf{U}, \mathbf{x}_{i}, \mathbf{z}_{i}, \boldsymbol{\beta}, \tau) = \frac{y_{i}\eta_{i}-b(\eta_{i})}{\tau} + c(y_{i}, \tau),$$
            where $\eta_{i} = \mathbf{x}^{T}_{i}\boldsymbol{\beta} + \mathbf{z}^{T}_{i}\mathbf{U}$ is the linear predictor now enhanced to include the random effects $\mathbf{U}$ via the terms $\mathbf{z}^{T}_{i}\mathbf{U}$
            \item In Normal distribution case, $$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\mathbf{U} + e$$
            here written in the familiar vector form $\mathbf{X} = (\mathbf{x}^{T}_{1}, \cdots, \mathbf{x}^{T}_{n})$ and $\mathbf{X} = (\mathbf{z}^{T}_{1}, \cdots, \mathbf{z}^{T}_{n})$
            
            $$\int \prod^{n}_{i=1} f_{Y_{i}|\mathbf{U}}f(\mathbf{u})d\mathbf{U}$$
        \end{itemize} 
        \end{frame}


       \begin{frame}{Accelerated Failure Model}
        \begin{itemize}
            \item Accelerated Failure Models comprise an important class of regression models for censored data. For the ease of random right censoring, the model is 
            $$\log T_{i} = \mathbf{x}^{T}_{i}\boldsymbol{\beta}, $$
            where we observe $Y_{i} = \min\left(\log T_{i}, \log R_{i}\right),$ and $R_{i}$ is a censoring time that is assumed independent of $T_{t}$
            \item The typical models for the errors $e_{i}$ are standard normal, logistic, or the logatithm of a standard exponential random variabl whose density is $f_{e} = e^{z}e^{-e^{z}}$.
            \item The likelihood for parameters $\boldsymbol{\beta}$ and $\sigma$ with using error density $f_{e}$ and distribution $F_{e}$ is 
            $$L(\boldsymbol{\beta}, \sigma|\{y_{i}, \delta_{i}, \mathbf{x}_{i}\}^{n}_{i=1}) = \prod^{n}_{i=1}\left[\frac{1}{\sigma}f_{e}(r_{i})\right]^{\delta_{i}}\left[1 - F_{e}(r_{i})\right]^{\delta_{i}},$$
            where $\delta_{i} = I(y_{i} = \log t_{i})$ and $r_{i} = (y_{i} - \mathbf{x}^{T}_{i}\boldsymbol{\beta})/\sigma.$ 
           
        \end{itemize}
    \end{frame}
    



\end{document}

